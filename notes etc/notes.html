<!DOCTYPE html>
<html lang="fr">
<head>
  <meta charset="UTF-8">
  <title>Comprendre et traiter la colinéarité parfaite dans notre modèle sur l’Overshoot Day pour garder la bonne spécification</title>
  <style>
    body {
      font-family: Georgia, "Times New Roman", serif;
      line-height: 1.6;
      margin: 40px;
      max-width: 900px;
      color: #111;
    }
    h1, h2, h3 {
      font-family: Arial, Helvetica, sans-serif;
      margin-top: 2em;
    }
    pre {
      background: #f5f5f5;
      padding: 1em;
      border: 1px solid #ddd;
      overflow-x: auto;
    }
    blockquote {
      margin-left: 20px;
      padding-left: 15px;
      border-left: 4px solid #ccc;
      font-style: italic;
    }
  </style>
</head>

<body>

<h1>Comprendre et traiter rigoureusement la colinéarité parfaite dans notre modèle sur l’Overshoot Day</h1>

<h2>1. Le point de départ : pourquoi la colinéarité me posait problème</h2>

<p>Au départ, je ne comprenais pas comment on pouvait dire que la multicolinéarité n’était <em>pas un problème</em>, alors que par définition :</p>

<blockquote>
la multicolinéarité signifie que certaines variables explicatives sont fortement (ou exactement) liées entre elles, ce qui empêche d’identifier l’effet individuel de chacune.
</blockquote>

<p>Dans un modèle linéaire classique, cela a des conséquences bien connues :</p>

<ul>
  <li>les coefficients deviennent instables,</li>
  <li>les erreurs standards explosent,</li>
  <li>l’interprétation individuelle des variables n’est plus possible.</li>
</ul>

<p>Dans notre projet sur l’Overshoot Day, on travaille précisément avec :</p>

<ul>
  <li>des empreintes écologiques par composante,</li>
  <li>des empreintes totales,</li>
  <li>des biocapacités par type,</li>
  <li>des biocapacités totales,</li>
</ul>

<p>donc avec des variables <strong>structurellement liées</strong> (par construction, certaines sont des sommes exactes des autres).</p>

<p>Quand j’ai soulevé le problème, la prof m’a répondu en substance :</p>

<blockquote>
« Dans notre cas, on s’en moque ».
</blockquote>

<p>Sur le moment, ça m’a paru un peu simpliste, mais en creusant j’ai compris <strong>pourquoi elle avait raison sur un point précis</strong>.</p>

<h2>2. ce que j'ai compris à propos de ce qu'a dit la prof</h2>

<p>En régression linéaire, ce qui détermine la prédiction n’est pas le vecteur des coefficients (β) pris isolément, mais le produit (Xβ).</p>

<p>S’il existe plusieurs vecteurs (β) tels que :</p>

<pre>
Xβ₁ = Xβ₂,
</pre>

<p>alors les prédictions sont strictement identiques, même si les coefficients sont très différents.</p>

<p>Or, en présence de colinéarité (et a fortiori de colinéarité parfaite), il existe <strong>une infinité de solutions</strong> pour (β) qui donnent exactement les mêmes valeurs ajustées.</p>

<p>Donc :</p>

<ul>
  <li>pour prédire l’Overshoot Day sur des données similaires,</li>
  <li>pour comparer des modèles en termes de (R²),</li>
  <li>pour produire une baseline prédictive,</li>
</ul>

<p>la colinéarité <strong>n’empêche pas le modèle de fonctionner</strong>.</p>

<p>En revanche, cela implique immédiatement une conséquence importante.</p>

<h2>3. Ce qu’on ne peut pas faire : interpréter les coefficients (feature importance)</h2>

<p>Même si la prédiction reste valide, <strong>on ne peut pas interpréter les coefficients individuellement</strong> quand il y a une forte colinéarité.</p>

<p>Donc, par exemple :</p>

<ul>
  <li>l’idée de Matteo de faire de la <em>feature importance</em> directement à partir des coefficients</li>
  <li>ou de comparer les effets marginaux des variables</li>
</ul>

<p>n’est <strong>pas mathématiquement défendable</strong> dans ce contexte.</p>

<p>À ce stade-là, j’ai donc laissé tomber :</p>

<ul>
  <li>la régularisation comme “solution miracle”,</li>
  <li>l’idée de rendre les coefficients interprétables individuellement.</li>
</ul>

<p>Mais il restait <strong>un problème fondamental</strong>.</p>

<h2>4. Pourquoi la colinéarité parfaite, elle, ne peut pas être ignorée</h2>

<p>La colinéarité parfaite n’est pas un problème “subjectif” ou “de confort d’interprétation”.</p>

<p>C’est un <strong>problème mathématique strict</strong>.</p>

<p>Dans un modèle linéaire :</p>

<pre>
β̂ = (XᵀX)⁻¹ Xᵀy
</pre>

<p>n’existe que si (XᵀX) est inversible, ce qui est équivalent à :</p>

<pre>
rang(X) = nombre de colonnes.
</pre>

<p>Si certaines colonnes sont des combinaisons linéaires exactes des autres :</p>

<ul>
  <li>la matrice est singulière,</li>
  <li>l’inverse n’existe pas,</li>
  <li>le problème n’a pas de solution unique.</li>
</ul>

<p>Donc, <strong>la colinéarité parfaite doit être traitée</strong>, que le logiciel nous le dise explicitement ou non.</p>

<p>Et c’est là que j’ai commencé à me poser la vraie question :</p>

<blockquote>
comment R et Python gèrent-ils ce problème, concrètement ?
</blockquote>

<!-- Le reste du texte est inclus exactement de la même manière -->

</body>
</html>
<h2>5. Le malaise central : ils le traitent différemment, sans vraiment l’expliquer</h2>

<p>Quand j’ai estimé le modèle complet :</p>

<ul>
  <li>R me renvoyait des alertes de singularité,</li>
  <li>certains coefficients devenaient <code>NA</code>,</li>
  <li><code>alias()</code> me montrait clairement des dépendances exactes.</li>
</ul>

<p>En Python, avec statsmodels :</p>

<ul>
  <li>le modèle s’ajustait quand même,</li>
  <li>j’avais juste un warning du type :</li>
</ul>

<blockquote>
“The smallest eigenvalue is ~1e-22”
</blockquote>

<ul>
  <li>mais aucun coefficient n’était explicitement supprimé.</li>
</ul>

<p>Et c’est là que quelque chose ne collait pas pour moi.</p>

<p>Si :</p>

<ul>
  <li>la colinéarité parfaite signifie que la matrice ne peut pas être inversée,</li>
  <li>et si Python continue quand même,</li>
</ul>

<p>alors <strong>comment est-ce possible mathématiquement ?</strong><br>
Et surtout : <strong>comment savoir si on est dans le cas “parfait” ou juste “très fort mais acceptable” ?</strong></p>

<h2>6. Comprendre le cadre théorique : OLS et inversion</h2>

<p>Je suis donc revenue à la théorie.</p>

<p>Le modèle OLS classique repose sur :</p>

<pre>
(XᵀX)⁻¹
</pre>

<p>Si le rang de (X) est strictement inférieur au nombre de colonnes :</p>

<ul>
  <li>l’inverse n’existe pas,</li>
  <li>il y a une infinité de solutions (β),</li>
  <li>les coefficients ne sont pas identifiables.</li>
</ul>

<p>Donc, si un logiciel “continue quand même”, il <strong>ne peut pas utiliser l’inverse classique</strong>.</p>

<p>Il utilise nécessairement <strong>une autre construction mathématique</strong>.</p>

<h2>7. Ce que fait R : décomposition QR et suppression de colonnes</h2>

<p>R utilise une <strong>décomposition QR avec pivoting</strong> :</p>

<pre>
X = QRP.
</pre>

<p>Les pivots diagonaux de (R) indiquent l’indépendance des colonnes :</p>

<ul>
  <li>pivot nul → dépendance linéaire exacte.</li>
</ul>

<p>Quand R détecte une dépendance :</p>

<ul>
  <li>il supprime certaines colonnes,</li>
  <li>marque leurs coefficients comme <code>NA</code>,</li>
  <li>restaure un rang plein,</li>
  <li>estime le modèle sur une sous-matrice identifiable.</li>
</ul>

<p>Donc R <strong>force l’identifiabilité</strong>, mais :</p>

<ul>
  <li>il fait un choix arbitraire,</li>
  <li>basé sur l’ordre des variables dans la formule,</li>
  <li>sans demander l’avis de l’utilisateur.</li>
</ul>

<p><code>alias()</code> rend ce choix lisible :
il exprime certaines variables comme combinaisons exactes des autres, avec des 0 et des 1 très clairs.</p>

<h2>8. Ce que fait Python / statsmodels : pseudo-inverse</h2>

<p>Statsmodels fait un choix complètement différent.</p>

<p>Il utilise la <strong>pseudo-inverse de Moore–Penrose</strong>, notée (X⁺), définie à partir de la SVD :</p>

<pre>
X = U Σ Vᵀ.
</pre>

<p>La solution calculée est :</p>

<pre>
β̂ = X⁺ y.
</pre>

<p>Conséquences :</p>

<ul>
  <li>statsmodels <strong>n’essaie pas d’inverser (XᵀX)</strong>,</li>
  <li>il <strong>ne supprime aucune colonne</strong>,</li>
  <li>il <strong>ne force pas le rang plein</strong>,</li>
  <li>il choisit une solution parmi l’infinité possible : celle de norme minimale.</li>
</ul>

<p>Donc :</p>

<ul>
  <li>les prédictions sont bien définies,</li>
  <li>le (R²) est valide,</li>
  <li>mais les coefficients ne sont pas identifiables individuellement.</li>
</ul>

<p>Et surtout :</p>

<blockquote>
statsmodels ne distingue pas explicitement colinéarité parfaite et quasi-parfaite.
</blockquote>

<h2>9. Pourquoi les warnings Python ne suffisent pas</h2>

<p>Les messages du type :</p>

<ul>
  <li>“smallest eigenvalue is ~1e-22”,</li>
  <li>“condition number is large”,</li>
</ul>

<p>indiquent seulement que la matrice est <strong>numériquement mal conditionnée</strong>.</p>

<p>Ils ne disent pas :</p>

<ul>
  <li>s’il y a colinéarité parfaite (rang strictement insuffisant),</li>
  <li>ou seulement une colinéarité très forte.</li>
</ul>

<p>Or, pour moi, la différence est cruciale :</p>

<ul>
  <li>quasi-parfaite → on peut suivre la prof et ne rien toucher,</li>
  <li>parfaite → le modèle n’est pas identifiable, il faut agir.</li>
</ul>

<h2>10. Le critère décisif : le rang de la matrice</h2>

<p>En lisant la documentation et les échanges des mainteneurs de statsmodels, j’ai trouvé la réponse claire :</p>

<blockquote>
“In programmatic use, the user can check the rank attribute of the model.”
</blockquote>

<p>Donc j’ai vérifié :</p>

<pre>
np.linalg.matrix_rank(X), X.shape[1]
</pre>

<p>Interprétation sans ambiguïté :</p>

<ul>
  <li>rang &lt; nombre de colonnes → colinéarité parfaite,</li>
  <li>rang = nombre de colonnes → pas de colinéarité parfaite.</li>
</ul>

<p>Et là, j’ai constaté que :</p>

<ul>
  <li>la plupart de mes spécifications n’étaient <strong>pas de rang plein</strong>,</li>
  <li>sauf celle avec uniquement les variables totales.</li>
</ul>

<p>Donc Python faisait bien le modèle, mais <strong>en utilisant une pseudo-inverse</strong>, pas un vrai OLS identifiable.</p>

<h2>11. Retour vers R : utiliser alias pour nettoyer rigoureusement</h2>

<p>À ce stade, j’ai décidé de procéder de manière rigoureuse :</p>

<ol>
  <li>Dans R :
    <ul>
      <li>j’ai ajusté le modèle complet,</li>
      <li>utilisé <code>alias()</code> pour identifier les dépendances exactes,</li>
      <li>supprimé les variables flaggées.</li>
    </ul>
  </li>
  <li>En Python :
    <ul>
      <li>j’ai ajusté exactement la même spécification,</li>
      <li>vérifié que :
        <pre>
rang = nombre de colonnes.
        </pre>
      </li>
    </ul>
  </li>
</ol>

<p>Résultat :</p>

<ul>
  <li>plus de colinéarité parfaite,</li>
  <li>statsmodels n’utilise plus de pseudo-inverse,</li>
  <li>les coefficients sont désormais <strong>uniquement définis</strong>.</li>
</ul>

<p>C’était exactement ce que je cherchais à vérifier.</p>

<h2>12. Pourquoi le warning subsiste (et pourquoi ce n’est plus le même problème)</h2>

<p>Le condition number reste élevé (~1e6), ce qui signifie :</p>

<ul>
  <li>forte colinéarité,</li>
  <li>mais <strong>pas parfaite</strong>.</li>
</ul>

<p>Mathématiquement :</p>

<ul>
  <li>(XᵀX) est inversible,</li>
  <li>mais mal conditionnée,</li>
  <li>les coefficients restent instables,</li>
  <li>les prédictions restent stables.</li>
</ul>


<h2>13. Pourquoi j’ai voulu comprendre comment <code>alias()</code> fonctionne</h2>

<p><code>alias()</code> me paraissait “évident” parce qu’il dit directement :</p>

<ul>
  <li>quelle variable est redondante,</li>
  <li>laquelle supprimer.</li>
</ul>

<p>Mais en essayant de le reproduire en Python, j’ai compris pourquoi.</p>

<p>R :</p>

<ul>
  <li>choisit une variable pivot,</li>
  <li>exprime une variable comme fonction des autres,</li>
  <li>donne une réponse <strong>orientée décision</strong>.</li>
</ul>

<p>Python (via la SVD) :</p>

<ul>
  <li>décrit l’espace entier des dépendances,</li>
  <li>ne choisit pas de pivot,</li>
  <li>ne dit pas “quoi enlever”.</li>
</ul>

<p>Les deux disent la même chose mathématiquement.
R ajoute une couche de décision.</p>

<h2>14. Le point clé : il n’existe pas de “bonne” variable à supprimer</h2>

<p>C’est la conclusion la plus importante.</p>

<p>Quand il y a colinéarité parfaite :</p>

<ul>
  <li>enlever n’importe quelle variable de la relation rétablit le rang plein,</li>
  <li>toutes les options sont mathématiquement équivalentes,</li>
  <li>le choix est <strong>substantif</strong>, pas statistique.</li>
</ul>

<p>R choisit automatiquement.
Python refuse de choisir.
Et c’est exactement pour ça que R est plus confortable, mais aussi plus opaque.</p>
<h2>16. Ce que ça implique si on veut faire ça nativement en Python</h2>


<p>Si on veut traiter la colinéarité parfaite <strong>directement en Python</strong>, sans passer par R, on <strong>ne peut pas simplement “répliquer alias() tel quel”</strong>.</p>


<ul>
  <li>l’équivalent mathématique d’<code>alias()</code> en Python (via SVD ou noyau de la matrice) décrit un <strong>espace de dépendances linéaires</strong>,</li>
  <li>cet espace contient <strong>toutes</strong> les combinaisons exactes possibles,</li>
  <li>mais il ne désigne <strong>aucune variable en particulier</strong> comme “celle à supprimer”.</li>
</ul>

<p>Autrement dit, Python me dit :</p>

<blockquote>
“Voici l’espace vectoriel des relations exactes entre tes variables.”
</blockquote>

<p>Alors que R, avec <code>alias()</code>, me dit :</p>

<blockquote>
“Dans cette spécification précise, ces variables-là sont non estimables. Supprime-les.”
</blockquote>

<p>Ce sont deux niveaux différents :</p>

<ul>
  <li>Python me donne la <strong>structure géométrique complète</strong>,</li>
  <li>R me donne une <strong>décision opérationnelle</strong>.</li>
</ul>

<p>Si on reste entièrement en Python, alors :</p>

<ul>
  <li>on peut détecter qu’il existe une colinéarité parfaite (via le rang),</li>
  <li>on peut identifier les <strong>familles de variables colinéaires</strong> (via le noyau / la SVD),</li>
  <li>mais on doit ensuite <strong>choisir nous-mêmes un représentant</strong> dans chaque famille.</li>
</ul>

<p>Concrètement, cela veut dire :</p>

<ul>
  <li>chaque relation de dépendance définit une <strong>classe d’équivalence</strong> de variables,</li>
  <li>toutes les variables de cette classe portent la même information prédictive,</li>
  <li>en garder une seule suffit à rétablir le rang plein.</li>
</ul>

<p>Mais :</p>

<ul>
  <li>le choix de laquelle garder <strong>n’est pas mathématique</strong>,</li>
  <li>il est <strong>substantif</strong>, lié au sens du projet.</li>
</ul>

<p>Par exemple, dans notre projet :</p>

<ul>
  <li>si une empreinte de cons est combinaison exacte d'autres empreintes de prod,</li>
  <li>on peut garder soit l'une ou l'autre,</li>
  <li>mais pas les deux simultanément et ce choix dépend de ce qu’on veut raconter.
